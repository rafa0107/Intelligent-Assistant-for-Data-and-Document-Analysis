{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a13bdecc",
   "metadata": {},
   "source": [
    "**# Importanto o Dataset e verificando o conteúdo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7709c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           issue_url  \\\n",
      "0  \"https://github.com/zhangyuanwei/node-images/i...   \n",
      "1     \"https://github.com/Microsoft/pxt/issues/2543\"   \n",
      "2  \"https://github.com/MatisiekPL/Czekolada/issue...   \n",
      "3  \"https://github.com/MatisiekPL/Czekolada/issue...   \n",
      "4  \"https://github.com/MatisiekPL/Czekolada/issue...   \n",
      "\n",
      "                                         issue_title  \\\n",
      "0  can't load the addon. issue to: https://github...   \n",
      "1  hcl accessibility a11yblocking a11ymas mas4.2....   \n",
      "2  issue 1265: issue 1264: issue 1261: issue 1260...   \n",
      "3  issue 1266: issue 1263: issue 1262: issue 1259...   \n",
      "4  issue 1288: issue 1285: issue 1284: issue 1281...   \n",
      "\n",
      "                                                body  \n",
      "0  can't load the addon. issue to: https://github...  \n",
      "1  user experience: user who depends on screen re...  \n",
      "2  ┆attachments: <a href= https:& x2f;& x2f;githu...  \n",
      "3  gitlo = github x trello\\n---\\nthis board is no...  \n",
      "4  ┆attachments: <a href= https:& x2f;& x2f;githu...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rafae\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords') #carrego stopwrds para filtragem NLP\n",
    "\n",
    "# Adiciona o caminho da pasta onde o arquivo 'dataset.py' está localizado\n",
    "# Isso pula a necessidade de mencionar a pasta com hífen no import\n",
    "caminho_raw = os.path.abspath(os.path.join('..', 'data', 'raw'))\n",
    "if caminho_raw not in sys.path:\n",
    "    sys.path.append(caminho_raw)\n",
    "\n",
    "from dataset import carregar_dados\n",
    "\n",
    "df = carregar_dados()\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0010a67a",
   "metadata": {},
   "source": [
    "**Processamento e limpeza dos dados**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "427d4c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média de palavras em url: 1.0, em title: 10.9025, em body: 38.63875\n",
      "Resultados para 'issue_url':\n",
      "- Valores Nulos (NaN): 0\n",
      "- Textos Vazios/Espaços: 0\n",
      "Resultados para 'clean_title':\n",
      "- Valores Nulos (NaN): 0\n",
      "- Textos Vazios/Espaços: 69\n",
      "Resultados para 'clean_body':\n",
      "- Valores Nulos (NaN): 0\n",
      "- Textos Vazios/Espaços: 4\n",
      "Palavras mais frequentes em titles: [('issu', 352), ('add', 35), ('error', 21), ('bug', 21), ('use', 18), ('set', 12), ('test', 10), ('line', 9), ('task', 9), ('jquery', 8), ('api', 8), ('fix', 6), ('training', 6), ('network', 6), ('app', 5), ('help', 5), ('npm', 5), ('load', 4), ('button', 4), ('warning', 4)]\n",
      "Palavras mais frequentes em body: [('close', 1237), ('add', 939), ('issues', 898), ('columns', 704), ('list', 540), ('comment', 538), ('column', 531), ('trello', 528), ('update', 367), ('via', 360), ('default', 360), ('move', 358), ('settings', 357), ('custom', 355), ('attachments', 352), ('matisiekpl', 352), ('czekolada', 352), ('gitlo', 352), ('board', 352), ('sync', 352)]\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join('..', 'data', 'raw')))\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'data', 'processed')))\n",
    "\n",
    "\n",
    "from dataset import verifica_vazios, verifica_frequentes\n",
    "from data_processing import clean_text\n",
    "\n",
    "df[\"clean_title\"] = df[\"issue_title\"].apply(clean_text)\n",
    "df[\"clean_body\"] = df[\"body\"].apply(clean_text)\n",
    "\n",
    "\n",
    "media_palavras_url = df[\"issue_url\"].str.split().str.len().mean()\n",
    "media_palavras_title = df[\"clean_title\"].str.split().str.len().mean()\n",
    "media_palavras_body = df[\"clean_body\"].str.split().str.len().mean()\n",
    "\n",
    "print(f\"Média de palavras em url: {media_palavras_url}, em title: {media_palavras_title}, em body: {media_palavras_body}\")\n",
    "\n",
    "url_vazio = verifica_vazios(df,\"issue_url\")\n",
    "titles_vazio = verifica_vazios(df,\"clean_title\")\n",
    "body_vazio = verifica_vazios(df,\"clean_body\")\n",
    "\n",
    "palavras_frequentes_titles = verifica_frequentes(df,\"clean_title\", 20)\n",
    "print(f\"Palavras mais frequentes em titles: {palavras_frequentes_titles}\")\n",
    "palavras_frequentes_body = verifica_frequentes(df,\"clean_body\", 20)\n",
    "print(f\"Palavras mais frequentes em body: {palavras_frequentes_body}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68051a23",
   "metadata": {},
   "source": [
    "\n",
    "   **Análise descritiva dos dados processados**\n",
    "\n",
    "- Média de palavras para entender quantos tokens irão ser processados pela LLM\n",
    "    - Quantidade de células vazias em todas as colunas\n",
    "        - não teve resultado de células vazias, todas preenchidas\n",
    "    - Títulos - tamanhos médio de 5,4 palavras\n",
    "    - Body - Aproximadamente 28 palavras\n",
    "- Ideal para:\n",
    "    - Embeddings\n",
    "    - Chunking Leve\n",
    "    - RAG Eficiente(baixo custo e boa semântica)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d86188",
   "metadata": {},
   "source": [
    "**Criação de Texto final**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4683ea7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 833.20it/s, Materializing param=pooler.dense.weight]                               \n",
      "BertModel LOAD REPORT from: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_body</th>\n",
       "      <th>final_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>load addon issue error lib libc version glibc ...</td>\n",
       "      <td>load addon issue error lib libc version glibc ...</td>\n",
       "      <td>Title: load addon issue error lib libc version...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hcl accessibility yblocking ymas mas hcl makec...</td>\n",
       "      <td>user experience user depends screen reader get...</td>\n",
       "      <td>Title: hcl accessibility yblocking ymas mas hc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>issue issue issue issue issue issue issue issu...</td>\n",
       "      <td>attachments github com matisiekpl czekolada is...</td>\n",
       "      <td>Title: issue issue issue issue issue issue iss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>issue issue issue issue issue issue issue issu...</td>\n",
       "      <td>gitlo github trello board linked update issue ...</td>\n",
       "      <td>Title: issue issue issue issue issue issue iss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>issue issue issue issue issue issue issue issu...</td>\n",
       "      <td>attachments github com matisiekpl czekolada is...</td>\n",
       "      <td>Title: issue issue issue issue issue issue iss...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         clean_title  \\\n",
       "0  load addon issue error lib libc version glibc ...   \n",
       "1  hcl accessibility yblocking ymas mas hcl makec...   \n",
       "2  issue issue issue issue issue issue issue issu...   \n",
       "3  issue issue issue issue issue issue issue issu...   \n",
       "4  issue issue issue issue issue issue issue issu...   \n",
       "\n",
       "                                          clean_body  \\\n",
       "0  load addon issue error lib libc version glibc ...   \n",
       "1  user experience user depends screen reader get...   \n",
       "2  attachments github com matisiekpl czekolada is...   \n",
       "3  gitlo github trello board linked update issue ...   \n",
       "4  attachments github com matisiekpl czekolada is...   \n",
       "\n",
       "                                          final_text  \n",
       "0  Title: load addon issue error lib libc version...  \n",
       "1  Title: hcl accessibility yblocking ymas mas hc...  \n",
       "2  Title: issue issue issue issue issue issue iss...  \n",
       "3  Title: issue issue issue issue issue issue iss...  \n",
       "4  Title: issue issue issue issue issue issue iss...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#criação de nova coluna para texto final que será direcionado ao embedding\n",
    "df[\"final_text\"] = (\n",
    "    \"Title: \" + df[\"clean_title\"] +\n",
    "    \". Body: \" + df[\"clean_body\"]\n",
    ")\n",
    "\n",
    "#carregando o modelo que será usado, modelo rápido e leve para projeto OBS: Uso da CPU pois GPU esta ultrapassada para o modelo\n",
    "model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\", device=\"cpu\") #usando o paraphrasal para poder perguntar em portugues\n",
    "df[[\"clean_title\", \"clean_body\", \"final_text\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c24c0a4",
   "metadata": {},
   "source": [
    "**Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74f18f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 25/25 [00:08<00:00,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#listando o texto final em variavel para ser codificada\n",
    "texts = df[\"final_text\"].tolist()\n",
    "\n",
    "#realização do embedding pelo modelo escolhido\n",
    "embeddings = model.encode( \n",
    "    texts,\n",
    "    show_progress_bar= True\n",
    ")\n",
    "\n",
    "#criação do array em np \n",
    "embeddings = np.array(embeddings)\n",
    "print(embeddings.shape)\n",
    "\n",
    "#persistindo os dados em formato npy e csv para nao necessitar de conversao novamente\n",
    "np.save(\"embeddings.npy\",embeddings)\n",
    "df.to_csv(\"issue_processed.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f82ff1",
   "metadata": {},
   "source": [
    "**Busca Semântica**\n",
    "\n",
    "- Primeiramente utilizarei busca ingênua para este caso, mais rápido e ideal para projetos pequenos.\n",
    "    - será utilizado semelhança de cossenos, variância de -1 a 1, sendo 1 o mais próximo.\n",
    "    - comparação de vetores do embedding, modelo utilizado de dimensão 384\n",
    "- Após implementado e projeto funconando, irei dar updgrade para utilizar índice vetorial\n",
    "- Devido a arquitetura proposta relacionado aos tipos de perguntas para o Chatbot, será implementado:\n",
    "    - Busca semântica\n",
    "    - Top k\n",
    "    - Limiar de similiaridade\n",
    "    Dessa forma será possível a resposta de perguntas analtícas e explicativas, não somente localizadoras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84ddcf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #ler diretamente esta célula sem precisar converter novamente os arquivos no embedding\n",
    "import numpy as np\n",
    "\n",
    "embeddings = np.load(\"embeddings.npy\")\n",
    "df = pd.read_csv(\"issue_processed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142573a2",
   "metadata": {},
   "source": [
    "**Exemplo de pergunta e formato para query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58b58c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800,)\n",
      "[798 496 508 670 789 523 607 672 584 709 647 588 482 365 737 480 642 382\n",
      " 381 794 550 738 404 530 704 712 713 383 546 636 577 707 409 655 520 463\n",
      " 459 488 598 467 618 473 524 398 570 745 678 369 627 371 486 561 386 622\n",
      " 626 706 470 541 658 679 516 792 769   0 602 640 674 665 551 721 666 582\n",
      " 555 601 590 406 778 727 576 744 518 357 359 478 400 431 651 500 747 532\n",
      " 564 770 723 515 765 781 603 513 785 606 635 569 621 517 580 657 566 685\n",
      " 372 464 370 702 438 641 595 407 690 356 768 556 428 680 471 736 362 498\n",
      " 729 725 533 493 728 774 648 355 364 683 687 630 505 629 562 506 579 726\n",
      " 527 746 559 605 714 720 684 358 654 695 477 617 451 531 732 628 631 772\n",
      " 447 671 660   1 536 384 444 632 439 499 675 762 761 760 759 756 757 758\n",
      " 763 771 688 686 681 739 643 395 363 710 691 682 542 563 689 458 507 420\n",
      " 661 667 777 494 634 623 410 694 367 501 697 481 485 663 696 474 716 604\n",
      " 385 624 717 452 783 460 380 743 402 609 593 529 374 396 547 649 472 730\n",
      " 422 405 589 775 748 742 437 645 613 432 715 619 646 616 731 668 568 423\n",
      " 610 548 574 484 578 611 457 637 465 456 503 440 417 537 368 540 483 633\n",
      " 453 600 571 399 673 504 366 354 773 390 797 375 573 639 664 553 608 511\n",
      " 644 466 656 587 581 408 512 567 389 799 413 592 394 790 575 676 659 703\n",
      " 391 554 545 650 620 705 677 560 796 722 491 490 591 779 360 614 791 525\n",
      " 724 435 397 795 735 445 526 544 699 701 509 539 393 549 558 430 522 793\n",
      " 538 766 543 594 597 711 767 392 534 693 514 615 489 776 455 441 387 586\n",
      " 401 585 448 379 599 764 652 528 552 436 442 495 662 596 535 378 510 425\n",
      " 708 373 719   3   7   8   9  10 166  15  14 170 172 164 339  37  34  33\n",
      "  32 340 350  55  41  42 336 312  43 304 305  65  67  64  62  58  56  78\n",
      "  79 290 292 294  74  75  76 293  71 308  73  72  68  69  85 250 253  88\n",
      " 270 278 271  91 289 288 286 287 285 301 302 298 300 254 257 248  89 244\n",
      " 246 249 263 262 261 260 284 283 282 280 279 267  87  83  84 264 265 134\n",
      " 212 144 220 214 190 148 147 145 191 227 111 109 230 125 124 115 114 133\n",
      " 322 321 325 326 328 324 224  50 223 103 107  98  96  94 104 241 239 237\n",
      " 233 231 119 117 259  16  28  24  22 333 334 335 353  25  31  29 156 331\n",
      " 330 163 161 157 140 141 142 143 173 174 150 154 151 203 122 199 198 193\n",
      " 192 137 136 184 188 179 178 177 182 176 123 135 217 205 209 204 158 487\n",
      " 449 700 612 780 361 421 377 492 740 718 741 557 376 754 752 751 750 753\n",
      " 755 427 572 434 788 433 782 469 519 446 625 443 583 784 424 733 521 450\n",
      " 479 414 734 638 475 476 692 502 416 468 565 426 653 669 497 749 418 269\n",
      " 272  36  38  39  40  12  11  13 295 296 297 228  44  45 226 225  30  20\n",
      "  21  23  26  27   2   4   5   6  19 165 167 168 169 171 175 152 153 155\n",
      " 159 160 162 180 181 183 185 186 187 189 138 139 146 149 194 195 196 197\n",
      " 200 201 202 126 127 128 129 130 131 132 206 207 208 210 211 213 215 216\n",
      " 218 219 221 222 229 108 110 112 113 116 118 120 121 232 234 235 236 238\n",
      " 240 242 243  92  93  95  97  99 100 101 102 105 106 245 247 251 252 255\n",
      " 256 258 266 268  80  63  81  82  86  90 273 274 275 276 277 281 299 291\n",
      "  54  70 303  57  66  61  77  60  59  48  47  46 314 313 311 310 309 307\n",
      " 306  17 345 346 347 348 349 351  18 338 352  49  51  52  53 315 316 317\n",
      " 318 319 320 327 323 329 332 342 341  35 337 343 344 411 454 698 403 388\n",
      " 419 461 429 412 786 787 415 462]\n",
      "[462 415 787 786 412 429 461 419 388 403 698 454 411 344 343 337  35 341\n",
      " 342 332 329 323 327 320 319 318 317 316 315  53]\n"
     ]
    }
   ],
   "source": [
    "from semantic_search import encode_query, cosine_similiarity_func\n",
    "\n",
    "pergunta = \"Quais são as issues mais comuns?\"\n",
    "query = encode_query(model, [pergunta])\n",
    "#print(query.shape)\n",
    "\n",
    "scores = cosine_similiarity_func(query,embeddings)\n",
    "scores = scores.flatten()\n",
    "scores_ordenados = np.argsort(scores)\n",
    "print(scores.shape)\n",
    "print(scores_ordenados)\n",
    "\n",
    "#fazendo busca top_k, depois refatorar montando em funcoes definidas\n",
    "# Atualizar o Readme e usar o topk como parametro para busca ao refatorar “Utilizamos top-k dinâmico para balancear cobertura semântica e precisão.”\n",
    "top_k = 30\n",
    "top_indices = scores_ordenados[-top_k:][::-1] #quero buscar os ultimos 20 valores e ordena-los em sequência maior para menor, pois np.arg retorna ordem crescente\n",
    "print(top_indices)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2040c41",
   "metadata": {},
   "source": [
    "**Implementando Limiar de similaridade**\n",
    "\n",
    " - Utilizei um limiar de similaridade cosseno ajustado empiricamente para garantir que apenas documentos semanticamente relevantes sejam utilizados como contexto para a LLM, reduzindo ruído e alucinação.\n",
    " - A princípio foi definido padrão = 0.30, após verifiquei de forma empirica os melhores valores para retornar respostas completas sem interferências de dados fora do padrão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "808f1706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'indice': np.int64(462), 'score': np.float32(0.41111237), 'text': 'Title: issues. Body: new issue test'}, {'indice': np.int64(415), 'score': np.float32(0.3567969), 'text': 'Title: issue. Body: got issue tissue'}, {'indice': np.int64(787), 'score': np.float32(0.32647505), 'text': 'Title: issue. Body: body new issue'}, {'indice': np.int64(786), 'score': np.float32(0.32647505), 'text': 'Title: issue. Body: body new issue'}, {'indice': np.int64(412), 'score': np.float32(0.32475835), 'text': 'Title: issue. Body: '}, {'indice': np.int64(429), 'score': np.float32(0.30329826), 'text': 'Title: issue. Body: testing issue issue'}]\n"
     ]
    }
   ],
   "source": [
    "#definir o limiar, fazer um loop retornando os melhores índices\n",
    "limiar = 0.30\n",
    "lista_final = []\n",
    "\n",
    "for i in range(len(top_indices)):\n",
    "    if scores[top_indices[i]] > limiar:\n",
    "        lista_final.append({\"indice\" : top_indices[i] ,\"score\" :scores[top_indices[i]], \"text\" : df.iloc[top_indices[i]][\"final_text\"]})\n",
    "\n",
    "print(lista_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbd2c9c",
   "metadata": {},
   "source": [
    "**Construção do contexto**\n",
    "\n",
    "- Utilizar dos dados retornados a partir do limiar e construir os textos mais similares para servir de entrada para LLM\n",
    "- estou retornando em texto todos os documentos analisados que passaram pelo limiar, poderia restringir a quantidade caso o número de tokens a ser utilizado na LLM seja limitado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4df7eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Contexto 1 | Similaridade: 0.411]\n",
      "Title: issues. Body: new issue test\n",
      "\n",
      "[Contexto 2 | Similaridade: 0.357]\n",
      "Title: issue. Body: got issue tissue\n",
      "\n",
      "[Contexto 3 | Similaridade: 0.326]\n",
      "Title: issue. Body: body new issue\n",
      "\n",
      "[Contexto 4 | Similaridade: 0.326]\n",
      "Title: issue. Body: body new issue\n",
      "\n",
      "[Contexto 5 | Similaridade: 0.325]\n",
      "Title: issue. Body: \n",
      "\n",
      "[Contexto 6 | Similaridade: 0.303]\n",
      "Title: issue. Body: testing issue issue\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_CHARS = 800  # por documento\n",
    "\n",
    "lista_final = sorted(\n",
    "    lista_final,\n",
    "    key=lambda x: x[\"score\"],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "contexto = \"\"\n",
    "limite = 28\n",
    "\n",
    "if len(lista_final) <= limite:\n",
    "    for i in range(len(lista_final)):\n",
    "        text_limited = lista_final[i][\"text\"][:MAX_CHARS]\n",
    "\n",
    "        contexto += (\n",
    "            f\"[Contexto {i+1} | \"\n",
    "            f\"Similaridade: {lista_final[i]['score']:.03f}]\\n\"\n",
    "            f\"{text_limited}\\n\\n\"\n",
    "        )\n",
    "\n",
    "else:\n",
    "    for i in range(limite):\n",
    "        text_limited = lista_final[i][\"text\"][:MAX_CHARS]\n",
    "\n",
    "        contexto += (\n",
    "            f\"[Contexto {i+1} | \"\n",
    "            f\"Similaridade: {lista_final[i]['score']:.03f}]\\n\"\n",
    "            f\"{text_limited}\\n\\n\"\n",
    "        )\n",
    "\n",
    "print(contexto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e7a471",
   "metadata": {},
   "source": [
    "**Definir tipos de perguntas para ajustar prompt e resposta**\n",
    "\n",
    "- Dividir as perguntas em 3 tipos\n",
    "    - (A) Podem ser respondidas claramente pelo modelo. Ex: \"quais tipos\", \"quais problemas\", \"sobre o que\", \"resuma\"..\n",
    "    - (B) Podem ser respondidas com ressalvas. Ex: “mais comuns”,“mais registrados”,“principais” ..\n",
    "    - (C) O Modelo pode alucinar e dar estatísticas erradas, não recomendado para o propósito da IA. Ex: \"quantos\", \"porcentagem\", \"frequência\", \"exata quantidade\" .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "603647e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipo da pergunta: QUALITATIVE\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join('..', 'src', 'prompts')))\n",
    "from rag_prompts import classify_question, build_direct_prompt, build_qualitative_prompt, build_out_of_scope_prompt, build_prompt\n",
    "\n",
    "QUESTION_TYPE_A = \"DIRECT\"\n",
    "QUESTION_TYPE_B = \"QUALITATIVE\" \n",
    "QUESTION_TYPE_C = \"OUT_OF_SCOPE\"\n",
    "\n",
    "question_type = classify_question(pergunta)\n",
    "print(\"Tipo da pergunta:\", question_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eba7b4",
   "metadata": {},
   "source": [
    "**Validador de prompts**\n",
    "\n",
    "- Antes de passar para LLM analisar, o validador irá verificar se a pergunta se enquadra num tópico a ser analisado ou pode ser respondido sem LLM\n",
    "- Irá atuar como Gatekeeper, reduzindo a quantidade de tokens analisados\n",
    "- Evita desperdício de Quota da LLM e redução de custo\n",
    "\n",
    "**Integração da LLM**\n",
    "\n",
    "- Estou usando Gemini devido a facilidade de obtenção de uma chave gratuita para estudantes\n",
    "- A LLM irá receber as informações retiradas do Dataset \n",
    "- Será utilizado engenharia de prompt para qualificar a análise da LLM\n",
    "- No projeto será implantado um scan de input para o usuário digitar sua propria chave api gemini e testar o programa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d63e8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As issues mais comuns parecem ser relativas a \"new issue\" e testes de \"issue\". Há também uma repetição da palavra \"issue\" em alguns títulos e corpos de relato.\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join('..', 'src', 'llm')))\n",
    "\n",
    "from context_validator import validate_context, static_fallback, insufficient_context_response\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from rag_prompts import build_prompt\n",
    "from gemini_client import generate_answer, answer_question\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "chave_api = os.getenv(\"GOOGLE_API_KEY\")\n",
    "resposta = answer_question(pergunta,contexto)\n",
    "print(resposta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
