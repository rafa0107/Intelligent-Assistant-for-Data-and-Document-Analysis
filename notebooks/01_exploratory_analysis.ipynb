{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a13bdecc",
   "metadata": {},
   "source": [
    "**# Importanto o Dataset e verificando o conteúdo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7709c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rafae\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           issue_url  \\\n",
      "0  \"https://github.com/zhangyuanwei/node-images/i...   \n",
      "1     \"https://github.com/Microsoft/pxt/issues/2543\"   \n",
      "2  \"https://github.com/MatisiekPL/Czekolada/issue...   \n",
      "3  \"https://github.com/MatisiekPL/Czekolada/issue...   \n",
      "4  \"https://github.com/MatisiekPL/Czekolada/issue...   \n",
      "\n",
      "                                         issue_title  \\\n",
      "0  can't load the addon. issue to: https://github...   \n",
      "1  hcl accessibility a11yblocking a11ymas mas4.2....   \n",
      "2  issue 1265: issue 1264: issue 1261: issue 1260...   \n",
      "3  issue 1266: issue 1263: issue 1262: issue 1259...   \n",
      "4  issue 1288: issue 1285: issue 1284: issue 1281...   \n",
      "\n",
      "                                                body  \n",
      "0  can't load the addon. issue to: https://github...  \n",
      "1  user experience: user who depends on screen re...  \n",
      "2  ┆attachments: <a href= https:& x2f;& x2f;githu...  \n",
      "3  gitlo = github x trello\\n---\\nthis board is no...  \n",
      "4  ┆attachments: <a href= https:& x2f;& x2f;githu...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rafae\\Desktop\\Portifolio\\AI_ASSISTANT_GITHUB\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rafae\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords') #carrego stopwrds para filtragem NLP\n",
    "\n",
    "# Adiciona o caminho da pasta onde o arquivo 'dataset.py' está localizado\n",
    "# Isso pula a necessidade de mencionar a pasta com hífen no import\n",
    "caminho_raw = os.path.abspath(os.path.join('..', 'data'))\n",
    "if caminho_raw not in sys.path:\n",
    "    sys.path.append(caminho_raw)\n",
    "\n",
    "from dataset import carregar_dados\n",
    "\n",
    "df = carregar_dados()\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0010a67a",
   "metadata": {},
   "source": [
    "**Processamento e limpeza dos dados**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "427d4c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média de palavras em url: 1.0, em title: 10.9025, em body: 38.63875\n",
      "Resultados para 'issue_url':\n",
      "- Valores Nulos (NaN): 0\n",
      "- Textos Vazios/Espaços: 0\n",
      "Resultados para 'clean_title':\n",
      "- Valores Nulos (NaN): 0\n",
      "- Textos Vazios/Espaços: 69\n",
      "Resultados para 'clean_body':\n",
      "- Valores Nulos (NaN): 0\n",
      "- Textos Vazios/Espaços: 4\n",
      "Palavras mais frequentes em titles: [('issu', 352), ('add', 35), ('error', 21), ('bug', 21), ('use', 18), ('set', 12), ('test', 10), ('line', 9), ('task', 9), ('jquery', 8), ('api', 8), ('fix', 6), ('training', 6), ('network', 6), ('app', 5), ('help', 5), ('npm', 5), ('load', 4), ('button', 4), ('warning', 4)]\n",
      "Palavras mais frequentes em body: [('close', 1237), ('add', 939), ('issues', 898), ('columns', 704), ('list', 540), ('comment', 538), ('column', 531), ('trello', 528), ('update', 367), ('via', 360), ('default', 360), ('move', 358), ('settings', 357), ('custom', 355), ('attachments', 352), ('matisiekpl', 352), ('czekolada', 352), ('gitlo', 352), ('board', 352), ('sync', 352)]\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join('..', 'data', 'processed')))\n",
    "\n",
    "\n",
    "from dataset import verifica_vazios, verifica_frequentes, clean_text\n",
    "\n",
    "df[\"clean_title\"] = df[\"issue_title\"].apply(clean_text)\n",
    "df[\"clean_body\"] = df[\"body\"].apply(clean_text)\n",
    "\n",
    "\n",
    "media_palavras_url = df[\"issue_url\"].str.split().str.len().mean()\n",
    "media_palavras_title = df[\"clean_title\"].str.split().str.len().mean()\n",
    "media_palavras_body = df[\"clean_body\"].str.split().str.len().mean()\n",
    "\n",
    "print(f\"Média de palavras em url: {media_palavras_url}, em title: {media_palavras_title}, em body: {media_palavras_body}\")\n",
    "\n",
    "url_vazio = verifica_vazios(df,\"issue_url\")\n",
    "titles_vazio = verifica_vazios(df,\"clean_title\")\n",
    "body_vazio = verifica_vazios(df,\"clean_body\")\n",
    "\n",
    "palavras_frequentes_titles = verifica_frequentes(df,\"clean_title\", 20)\n",
    "print(f\"Palavras mais frequentes em titles: {palavras_frequentes_titles}\")\n",
    "palavras_frequentes_body = verifica_frequentes(df,\"clean_body\", 20)\n",
    "print(f\"Palavras mais frequentes em body: {palavras_frequentes_body}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68051a23",
   "metadata": {},
   "source": [
    "\n",
    "   **Análise descritiva dos dados processados**\n",
    "\n",
    "- Média de palavras para entender quantos tokens irão ser processados pela LLM\n",
    "    - Quantidade de células vazias em todas as colunas\n",
    "        - não teve resultado de células vazias, todas preenchidas\n",
    "    - Títulos - tamanhos médio de 5,4 palavras\n",
    "    - Body - Aproximadamente 28 palavras\n",
    "- Ideal para:\n",
    "    - Embeddings\n",
    "    - Chunking Leve\n",
    "    - RAG Eficiente(baixo custo e boa semântica)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d86188",
   "metadata": {},
   "source": [
    "**Criação de Texto final**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4683ea7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 836.54it/s, Materializing param=pooler.dense.weight]                               \n",
      "BertModel LOAD REPORT from: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#criação de nova coluna para texto final que será direcionado ao embedding\n",
    "df[\"final_text\"] = (\n",
    "    \"Title: \" + df[\"clean_title\"] +\n",
    "    \". Body: \" + df[\"clean_body\"]\n",
    ")\n",
    "\n",
    "#carregando o modelo que será usado, modelo rápido e leve para projeto OBS: Uso da CPU pois GPU esta ultrapassada para o modelo\n",
    "model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\", device=\"cpu\") #usando o paraphrasal para poder perguntar em portugues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02e3f61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rafae\\Desktop\\Portifolio\\AI_ASSISTANT_GITHUB\\Intelligent-Assistant-for-Data-and-Document-Analysis\\processed\\embeddings.npy\n",
      "c:\\Users\\rafae\\Desktop\\Portifolio\\AI_ASSISTANT_GITHUB\\Intelligent-Assistant-for-Data-and-Document-Analysis\\issue_processed.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path_embeddings = Path(os.getcwd()[:-9]) /  \"processed\" / \"embeddings.npy\" \n",
    "path_issue_processed = os.getcwd()[:-9] + \"issue_processed.csv\"\n",
    "\n",
    "print(path_embeddings)\n",
    "print(path_issue_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c24c0a4",
   "metadata": {},
   "source": [
    "**Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74f18f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 25/25 [00:08<00:00,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "path_embeddings = Path(os.getcwd()[:-9]) / \"data\" /  \"processed\" / \"embeddings.npy\"\n",
    "path_issue_processed = Path(os.getcwd()[:-9]) / \"data\" /  \"processed\" / \"issue_processed.csv\"\n",
    "\n",
    "#listando o texto final em variavel para ser codificada\n",
    "texts = df[\"final_text\"].tolist()\n",
    "\n",
    "#realização do embedding pelo modelo escolhido\n",
    "embeddings = model.encode( \n",
    "    texts,\n",
    "    show_progress_bar= True\n",
    ")\n",
    "\n",
    "#criação do array em np \n",
    "embeddings = np.array(embeddings)\n",
    "print(embeddings.shape)\n",
    "\n",
    "#persistindo os dados em formato npy e csv para nao necessitar de conversao novamente\n",
    "np.save(path_embeddings,embeddings)\n",
    "df.to_csv(path_issue_processed, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "982e5003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rafae\\Desktop\\Portifolio\\AI_ASSISTANT_GITHUB\\Intelligent-Assistant-for-Data-and-Document-Analysis\\notebooks\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f82ff1",
   "metadata": {},
   "source": [
    "**Busca Semântica**\n",
    "\n",
    "- Primeiramente utilizarei busca ingênua para este caso, mais rápido e ideal para projetos pequenos.\n",
    "    - será utilizado semelhança de cossenos, variância de -1 a 1, sendo 1 o mais próximo.\n",
    "    - comparação de vetores do embedding, modelo utilizado de dimensão 384\n",
    "- Após implementado e projeto funconando, irei dar updgrade para utilizar índice vetorial\n",
    "- Devido a arquitetura proposta relacionado aos tipos de perguntas para o Chatbot, será implementado:\n",
    "    - Busca semântica\n",
    "    - Top k\n",
    "    - Limiar de similiaridade\n",
    "    Dessa forma será possível a resposta de perguntas analtícas e explicativas, não somente localizadoras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ddcf07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✨ Sucesso! Arquivos carregados via caminho absoluto.\n",
      "Shape dos Embeddings: (800, 384)\n",
      "Primeiras linhas do DataFrame:\n",
      "                                           issue_url  \\\n",
      "0  \"https://github.com/zhangyuanwei/node-images/i...   \n",
      "1     \"https://github.com/Microsoft/pxt/issues/2543\"   \n",
      "\n",
      "                                         issue_title  \\\n",
      "0  can't load the addon. issue to: https://github...   \n",
      "1  hcl accessibility a11yblocking a11ymas mas4.2....   \n",
      "\n",
      "                                                body  \\\n",
      "0  can't load the addon. issue to: https://github...   \n",
      "1  user experience: user who depends on screen re...   \n",
      "\n",
      "                                         clean_title  \\\n",
      "0  load addon issue error lib libc version glibc ...   \n",
      "1  hcl accessibility yblocking ymas mas hcl makec...   \n",
      "\n",
      "                                          clean_body  \\\n",
      "0  load addon issue error lib libc version glibc ...   \n",
      "1  user experience user depends screen reader get...   \n",
      "\n",
      "                                          final_text  \n",
      "0  Title: load addon issue error lib libc version...  \n",
      "1  Title: hcl accessibility yblocking ymas mas hc...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Caminhos absolutos apenas para testes\n",
    "path_npy = r\"C:\\Users\\rafae\\Desktop\\Portifolio\\AI_ASSISTANT_GITHUB\\Intelligent-Assistant-for-Data-and-Document-Analysis\\data\\processed\\embeddings.npy\"\n",
    "path_csv = r\"C:\\Users\\rafae\\Desktop\\Portifolio\\AI_ASSISTANT_GITHUB\\Intelligent-Assistant-for-Data-and-Document-Analysis\\data\\processed\\issue_processed.csv\"\n",
    "\n",
    "try:\n",
    "    embeddings = np.load(path_npy)\n",
    "    df = pd.read_csv(path_csv)\n",
    "    print(\"✨ Sucesso! Arquivos carregados via caminho absoluto.\")\n",
    "    \n",
    "    # Teste rápido de visualização\n",
    "    print(f\"Shape dos Embeddings: {embeddings.shape}\")\n",
    "    print(f\"Primeiras linhas do DataFrame:\\n{df.head(2)}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ Erro: O arquivo não foi encontrado no local especificado.\\n{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142573a2",
   "metadata": {},
   "source": [
    "**Exemplo de pergunta e formato para query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58b58c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rafae\\Desktop\\Portifolio\\AI_ASSISTANT_GITHUB\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 833.82it/s, Materializing param=pooler.dense.weight]                               \n",
      "BertModel LOAD REPORT from: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800,)\n",
      "[697 496 603 745 622 499 500 564 703 665 576 534 785 640 459 789 470 482\n",
      " 546 474 537 469 478 788 402 460 518 517 710 467 625 560 498 610 452 720\n",
      " 398 621 747 731 431 704 727 793 435 357 646 420 696 392 511 570 598 716\n",
      " 529 449 779 515 520 464 626 602 664 508 709 584 512 648 481 714 561 740\n",
      " 495 632 591 623 624 590 551 365 391 516 634 773 367 397 780 422 577   1\n",
      " 404 530 531 362 566 796 668 699 477 439 501 458 463 627 631 382 361 616\n",
      " 519 456 644 413 438 712 609 690 368 521 409 542 765 600 383 642 790 574\n",
      " 387 514 707 550 376 480 629 562 505 506 630 657 388 728 746 509 430 782\n",
      " 633 615 389 523 651 381 504 484 372 354 472 702 713 679 638 726 555 502\n",
      " 799 488 457 370 647 769 652 721 676 667 685 733 628 669 674 582 533 675\n",
      " 355 573 766 619 578 794 487 620 741 777 744 694 507 586 743 541 489 661\n",
      " 408 599 604   0 407 725 594 406 605 543 447 705 639 468 756 763 760 757\n",
      " 759 758 762 761 471 559 768 798 792 485 589 593 553 379 706 532 426 371\n",
      " 395 524 423 770 579 781 437 695 358 742 737 399 547 536 641 677 444 567\n",
      " 666 476 678 527 776 680 492 385 738 575 513 483 715 607 748 479 461 466\n",
      " 772 687 683 719 455 434 635 448 544 490 491 739 400 670 595 552 722 693\n",
      " 565 689 778 708 660 374 672 692 369 486 410 558 684 373 585 681 686 688\n",
      " 405 465 732 786 787 393 643 718 473 440 735 771 662 386 617  16  22  24\n",
      "  25  15 340 339 336 335 334 333  43  42  41  37  34  33  58  56  55  50\n",
      " 331 330 328 326 325 324 322 321  69  68  67  65  64  62  79  78  76  75\n",
      "  74  73  72  71 353 312  94  91  89  88  87  85  84  83 308 305 304 302\n",
      " 301 111 109 107 104 103  98  96 122 119 117 115 114 300 298 294 293 292\n",
      " 290 289 288 125 124 123 143 142 141 140 137 136 135 134 133 287 286 285\n",
      " 156 154 151 150 148 147 145 144 284 283 282 161 280 279 278 157 158 174\n",
      " 173 267 172 170 166 164 163 271 270 179 188 184 182 177 178 176 193 237\n",
      " 192 191 190 205 204 203 199 198 265 264 263 262 261 260 259 257 254 253\n",
      " 223 220 217 214 212 209 250 249 248 246 244 241 239  28 233 231 230 227\n",
      " 224  14  10   9   8   7   3 350  32  31  29 618 556 428 568 682 691 612\n",
      " 580 525 363 729 673 401 510 783 614 493 775 592 701 711 654 540 655 563\n",
      " 528 378 436 581 611 784 606 441 417 503 764 791 613 588 364 394 767 415\n",
      " 659 734 795 587 663 658 443 526 454 671 375 412 649 411 442 717 403 475\n",
      " 700 396  36 228 272 297 295 296 226  30  13 225  38  39 269  11  12  40\n",
      "  44  45 348 347 346 345 344 343 342 341 338 337  47  46   5   6   4   2\n",
      " 189 351 349 266 268 277 165 167 175 171 169 168 159 185 186 187 194 180\n",
      " 181 183 160 273 274 275 162 276 129 128  97  99 100 101 306 146 127 281\n",
      " 138 139 131 132 202 207 206 126 149 152 153 155 307 113 112 299 120 121\n",
      " 118 116 255 256 258 195 196 197 200 201 130 291 211 210 215 216 218 213\n",
      " 219 221 329 332 242 243 245 320 352 319 318 252 222 251  77  70  63  82\n",
      "  86  90  92  80  81  61 309  95  93 310 315 311 313 314 102 317 316 105\n",
      " 110 108 106 303  26  27  17  19  20  21  18 234  23 229 232 238  53  54\n",
      " 208 247  35  66  60 323 327 236 235 240  48  49  51  52  57  59 451 418\n",
      " 366 535 608 636 557 377 572 637 601 421 723 653 384 416 424 453 569 446\n",
      " 445 645 427 360 730 548 774 597 359 797 749 414 724 380 419 425 429 650\n",
      " 545 554 433 583 390 750 753 751 755 754 752 432 450 698 522 497 462 596\n",
      " 656 549 356 494 538 539 736 571]\n",
      "[571 736 539 538 494 356 549 656 596 462 497 522 698 450 432 752 754 755\n",
      " 751 753 750 390 583 433 554 545 650 429 425 419]\n"
     ]
    }
   ],
   "source": [
    "#importando as libs novamente apenas para teste na refatoracao\n",
    "import sys\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src', 'rag')))\n",
    "from retriever import encode_query, cosine_similiarity_func\n",
    "from context_builder import top_k_index\n",
    "\n",
    "pergunta = \"Quais sao os erros mais registrados no documento?\"\n",
    "query = encode_query(SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\", device=\"cpu\"), [pergunta])\n",
    "#print(query.shape)\n",
    "\n",
    "scores = cosine_similiarity_func(query,embeddings)\n",
    "scores = scores.flatten()\n",
    "scores_ordenados = np.argsort(scores)\n",
    "print(scores.shape)\n",
    "print(scores_ordenados)\n",
    "\n",
    "#fazendo busca top_k, depois refatorar montando em funcoes definidas\n",
    "# Atualizar o Readme e usar o topk como parametro para busca ao refatorar “Utilizamos top-k dinâmico para balancear cobertura semântica e precisão.”\n",
    "top_k = 30\n",
    "top_indices = top_k_index(top_k, scores_ordenados)\n",
    "print(top_indices)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2040c41",
   "metadata": {},
   "source": [
    "**Implementando Limiar de similaridade**\n",
    "\n",
    " - Utilizei um limiar de similaridade cosseno ajustado empiricamente para garantir que apenas documentos semanticamente relevantes sejam utilizados como contexto para a LLM, reduzindo ruído e alucinação.\n",
    " - A princípio foi definido padrão = 0.30, após verifiquei de forma empirica os melhores valores para retornar respostas completas sem interferências de dados fora do padrão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "808f1706",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'check_semantic_threshold' from 'retriever' (c:\\Users\\rafae\\Desktop\\Portifolio\\AI_ASSISTANT_GITHUB\\Intelligent-Assistant-for-Data-and-Document-Analysis\\src\\rag\\retriever.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mretriever\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_semantic_threshold\n\u001b[32m      3\u001b[39m limiar = \u001b[32m0.30\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m#funcao bsuca as informações dos scores de todo o dataframe atraves dos top 20 indices, e retorna seu conteudo numa lista \u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'check_semantic_threshold' from 'retriever' (c:\\Users\\rafae\\Desktop\\Portifolio\\AI_ASSISTANT_GITHUB\\Intelligent-Assistant-for-Data-and-Document-Analysis\\src\\rag\\retriever.py)"
     ]
    }
   ],
   "source": [
    "from retriever import check_semantic_threshold\n",
    "\n",
    "limiar = 0.30\n",
    "#funcao bsuca as informações dos scores de todo o dataframe atraves dos top 20 indices, e retorna seu conteudo numa lista \n",
    "final_list = check_semantic_threshold(scores, top_indices, limiar, df)\n",
    "print(final_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbd2c9c",
   "metadata": {},
   "source": [
    "**Construção do contexto**\n",
    "\n",
    "- Utilizar dos dados retornados a partir do limiar e construir os textos mais similares para servir de entrada para LLM\n",
    "- estou retornando em texto todos os documentos analisados que passaram pelo limiar, poderia restringir a quantidade caso o número de tokens a ser utilizado na LLM seja limitado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4df7eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Contexto 1 | Similaridade: 0.455]\n",
      "Title: . Body: according demo code returns standard errors according code column naming seems return standard deviations meantime assuming model returns standard deviations using boot boot empirical standard errors slow already including less unwieldy method computing kind standard errors report paper would happy clarification also information getting code work kept running error error abs max beta beta tol maxit missing value true false needed ultimately got around explicitly setting intercept input formula model matrix computed model matrix zero\n",
      "\n",
      "[Contexto 2 | Similaridade: 0.421]\n",
      "Title: null. Body: bulk insert seemed successful according log however checked database data inserted might cause book import books\n",
      "\n",
      "[Contexto 3 | Similaridade: 0.418]\n",
      "Title: insert. Body: organization name individual dcma organization type document reference include section paragraph number executive summary comment include rationale comment insert suggested change read requirement organization type federal industry academia self\n",
      "\n",
      "[Contexto 4 | Similaridade: 0.410]\n",
      "Title: insert. Body: organization name individual dcma organization type document reference include section paragraph number comment include rationale comment insert suggested change read owned patient organization type federal industry academia self\n",
      "\n",
      "[Contexto 5 | Similaridade: 0.402]\n",
      "Title: . Body: found two problems used lua function test print print end test lua function test local error compilechunk print end test thanks help\n",
      "\n",
      "[Contexto 6 | Similaridade: 0.390]\n",
      "Title: invalid. Body: judge eee\n",
      "\n",
      "[Contexto 7 | Similaridade: 0.386]\n",
      "Title: line. Body: brf number crashes impacted devices lot information crash crashlytics com\n",
      "\n",
      "[Contexto 8 | Similaridade: 0.379]\n",
      "Title: bug log. Body: mar europe berlin php fatal error uncaught error call member function query null inetpub stack trace inetpub profile user showforentity object entity inetpub profile user displaytabcontentforitem object entity inetpub commonglpi displaystandardtab object entity profile user array main thrown inetpub line mar europe berlin php fatal error uncaught zend exception parseexception unknown invalid parser rule nplurals integer plural expression inetpub stack trace inetpub zend translator plural rule fromstring nplurals integ inetpub zend translator loader gettext load inetpub inetpub zend translator translator loadmessagesfromfiles mydashboard inetpub zend translator translator loadmessages mydashboard inetpub zend translator translator gettr inetpub line\n",
      "\n",
      "[Contexto 9 | Similaridade: 0.377]\n",
      "Title: line. Body: number crashes impacted devices lot information crash crashlytics com\n",
      "\n",
      "[Contexto 10 | Similaridade: 0.373]\n",
      "Title: issues. Body: new issue test\n",
      "\n",
      "[Contexto 11 | Similaridade: 0.367]\n",
      "Title: add faq. Body: answer common questions countries data data gathered language deduced report error suggest feature epurpose project etc\n",
      "\n",
      "[Contexto 12 | Similaridade: 0.366]\n",
      "Title: ent dec. Body: entero decimal entero los ejemplos dice ent dec dec talvez error asi\n",
      "\n",
      "[Contexto 13 | Similaridade: 0.361]\n",
      "Title: fix. Body: still lots problems needs thorough testing launch\n",
      "\n",
      "[Contexto 14 | Similaridade: 0.358]\n",
      "Title: adress. Body: old needs adressed properly probably useful current state code could likely used one way another nemerald adress issue since code stale branch start\n",
      "\n",
      "[Contexto 15 | Similaridade: 0.357]\n",
      "Title: void. Body: actually void data structures placeholders everything passed arguments pedrohsoares wykthor btracker nicholas arqui let found\n",
      "\n",
      "[Contexto 16 | Similaridade: 0.352]\n",
      "Title: issue. Body: bug number\n",
      "\n",
      "[Contexto 17 | Similaridade: 0.352]\n",
      "Title: issue. Body: bug number\n",
      "\n",
      "[Contexto 18 | Similaridade: 0.352]\n",
      "Title: issue. Body: bug number\n",
      "\n",
      "[Contexto 19 | Similaridade: 0.352]\n",
      "Title: issue. Body: bug number\n",
      "\n",
      "[Contexto 20 | Similaridade: 0.352]\n",
      "Title: issue. Body: bug number\n",
      "\n",
      "[Contexto 21 | Similaridade: 0.352]\n",
      "Title: issue. Body: bug number\n",
      "\n",
      "[Contexto 22 | Similaridade: 0.351]\n",
      "Title: task. Body: ronitron complete form validations registrations form done closed\n",
      "\n",
      "[Contexto 23 | Similaridade: 0.343]\n",
      "Title: tests. Body: est dommage que fichier soit vide surtout quand dans ton historique commits des trucs genre fix machin truc framework pour les tests est super bien foutu permet vraiment faire des choses assez bal zes alors sans aller jusqu tout tester encore que aide vraiment mettre moins mettre les bugs que rencontres\n",
      "\n",
      "[Contexto 24 | Similaridade: 0.340]\n",
      "Title: req. Body: restricciones tiene para hacer esas ediciones modificaciones los datos del inventario\n",
      "\n",
      "[Contexto 25 | Similaridade: 0.340]\n",
      "Title: line. Body: com number crashes impacted devices lot information crash crashlytics com\n",
      "\n",
      "[Contexto 26 | Similaridade: 0.340]\n",
      "Title: line. Body: com number crashes impacted devices lot information crash crashlytics com\n",
      "\n",
      "[Contexto 27 | Similaridade: 0.340]\n",
      "Title: line. Body: com number crashes impacted devices lot information crash crashlytics com\n",
      "\n",
      "[Contexto 28 | Similaridade: 0.339]\n",
      "Title: issue. Body: testing issue issue\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from context_builder import context_builder\n",
    "\n",
    "contexto_final = context_builder(800, final_list, 28)\n",
    "print(contexto_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e7a471",
   "metadata": {},
   "source": [
    "**Definir tipos de perguntas para ajustar prompt e resposta**\n",
    "\n",
    "- Dividir as perguntas em 3 tipos\n",
    "    - (A) Podem ser respondidas claramente pelo modelo. Ex: \"quais tipos\", \"quais problemas\", \"sobre o que\", \"resuma\"..\n",
    "    - (B) Podem ser respondidas com ressalvas. Ex: “mais comuns”,“mais registrados”,“principais” ..\n",
    "    - (C) O Modelo pode alucinar e dar estatísticas erradas, não recomendado para o propósito da IA. Ex: \"quantos\", \"porcentagem\", \"frequência\", \"exata quantidade\" .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "603647e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipo da pergunta: QUALITATIVE\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join('..', 'src', 'prompts')))\n",
    "\n",
    "\n",
    "from validator import classify_question\n",
    "from rag_prompts import build_direct_prompt, build_qualitative_prompt, build_out_of_scope_prompt, build_prompt\n",
    "\n",
    "question_type = classify_question(pergunta)\n",
    "print(\"Tipo da pergunta:\", question_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eba7b4",
   "metadata": {},
   "source": [
    "**Validador de prompts**\n",
    "\n",
    "- Antes de passar para LLM analisar, o validador irá verificar se a pergunta se enquadra num tópico a ser analisado ou pode ser respondido sem LLM\n",
    "- Irá atuar como Gatekeeper, reduzindo a quantidade de tokens analisados\n",
    "- Evita desperdício de Quota da LLM e redução de custo\n",
    "\n",
    "**Integração da LLM**\n",
    "\n",
    "- Estou usando Gemini devido a facilidade de obtenção de uma chave gratuita para estudantes\n",
    "- A LLM irá receber as informações retiradas do Dataset \n",
    "- Será utilizado engenharia de prompt para qualificar a análise da LLM\n",
    "- No projeto será implantado um scan de input para o usuário digitar sua propria chave api gemini e testar o programa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d63e8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Os relatórios de problemas identificados demonstram os seguintes padrões e temas recorrentes:\n",
      "\n",
      "*   **Erros de Cálculo e Estatística:** Vários relatos mencionam problemas com erros padrão, desvios padrão e métodos de computação para obtê-los, inclusive em contextos de código de demonstração e modelos.\n",
      "*   **Problemas de Inserção de Dados:** Há relatos sobre falhas ou sucesso aparente na inserção de dados, como em livros em um banco de dados, e também sobre a necessidade de inserir informações específicas (nomes de organizações, tipos, etc.) em sistemas.\n",
      "*   **Erros de Código e Compilação:** Existem menções a erros de compilação de código, falhas em funções de Lua e erros fatais em PHP (como \"call to a member function query on null\").\n",
      "*   **Falhas e Travamentos (Crashes):** Relatos indicam travamentos em dispositivos, com menção a informações de crashlytics e números associados a esses eventos.\n",
      "*   **Problemas de Teste e Correção:** Há discussões sobre a necessidade de testes rigorosos, a presença de múltiplos problemas que precisam ser corrigidos antes do lançamento, e menções a \"bugs\" e \"issues\" em geral.\n",
      "*   **Validações e Restrições:** Algumas entradas abordam validações de formulários e restrições para edições de dados.\n",
      "*   **Questões de Tradução/Interpretação:** Existe uma menção a um possível erro em exemplos de \"entero decimal\".\n",
      "\n",
      "Temas mais genéricos como \"issue\" e \"bug\" aparecem repetidamente, indicando a natureza geral de problemas que precisam ser abordados.\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join('..', 'src', 'llm')))\n",
    "\n",
    "from validator import validate_context, static_fallback, insufficient_context_response\n",
    "from rag_prompts import build_prompt\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from gemini_client import generate_answer, answer_question\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "chave_api = os.getenv(\"GOOGLE_API_KEY\")\n",
    "resposta = answer_question(pergunta,contexto_final)\n",
    "print(resposta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
