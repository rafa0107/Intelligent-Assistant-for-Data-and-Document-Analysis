{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a13bdecc",
   "metadata": {},
   "source": [
    "**# Importanto o Dataset e verificando o conteúdo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7709c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rafae\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           issue_url  \\\n",
      "0  \"https://github.com/zhangyuanwei/node-images/i...   \n",
      "1     \"https://github.com/Microsoft/pxt/issues/2543\"   \n",
      "2  \"https://github.com/MatisiekPL/Czekolada/issue...   \n",
      "3  \"https://github.com/MatisiekPL/Czekolada/issue...   \n",
      "4  \"https://github.com/MatisiekPL/Czekolada/issue...   \n",
      "\n",
      "                                         issue_title  \\\n",
      "0  can't load the addon. issue to: https://github...   \n",
      "1  hcl accessibility a11yblocking a11ymas mas4.2....   \n",
      "2  issue 1265: issue 1264: issue 1261: issue 1260...   \n",
      "3  issue 1266: issue 1263: issue 1262: issue 1259...   \n",
      "4  issue 1288: issue 1285: issue 1284: issue 1281...   \n",
      "\n",
      "                                                body  \n",
      "0  can't load the addon. issue to: https://github...  \n",
      "1  user experience: user who depends on screen re...  \n",
      "2  ┆attachments: <a href= https:& x2f;& x2f;githu...  \n",
      "3  gitlo = github x trello\\n---\\nthis board is no...  \n",
      "4  ┆attachments: <a href= https:& x2f;& x2f;githu...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rafae\\Desktop\\Portifolio\\AI_ASSISTANT_GITHUB\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rafae\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords') #carrego stopwrds para filtragem NLP\n",
    "\n",
    "# Adiciona o caminho da pasta onde o arquivo 'dataset.py' está localizado\n",
    "# Isso pula a necessidade de mencionar a pasta com hífen no import\n",
    "caminho_raw = os.path.abspath(os.path.join('..', 'data', 'raw'))\n",
    "if caminho_raw not in sys.path:\n",
    "    sys.path.append(caminho_raw)\n",
    "\n",
    "from dataset import carregar_dados\n",
    "\n",
    "df = carregar_dados()\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0010a67a",
   "metadata": {},
   "source": [
    "**Processamento e limpeza dos dados**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "427d4c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média de palavras em url: 1.0, em title: 22.066666666666666, em body: 56.526666666666664\n",
      "Resultados para 'issue_url':\n",
      "- Valores Nulos (NaN): 0\n",
      "- Textos Vazios/Espaços: 0\n",
      "Resultados para 'clean_title':\n",
      "- Valores Nulos (NaN): 0\n",
      "- Textos Vazios/Espaços: 0\n",
      "Resultados para 'clean_body':\n",
      "- Valores Nulos (NaN): 0\n",
      "- Textos Vazios/Espaços: 0\n",
      "Palavras mais frequentes em titles: [('issu', 148), ('fileserver', 2), ('node', 2), ('hcl', 2), ('side', 2), ('documentation', 2), ('button', 2), ('load', 1), ('addon', 1), ('error', 1), ('lib', 1), ('libc', 1), ('version', 1), ('glibc', 1), ('found', 1), ('required', 1), ('usr', 1), ('local', 1), ('app', 1), ('taf', 1)]\n",
      "Palavras mais frequentes em body: [('close', 525), ('add', 375), ('issues', 373), ('columns', 300), ('trello', 225), ('column', 225), ('comment', 225), ('list', 225), ('gitlo', 150), ('board', 150), ('update', 150), ('sync', 150), ('via', 150), ('card', 150), ('move', 150), ('cards', 150), ('custom', 150), ('default', 150), ('settings', 150), ('attachments', 148)]\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join('..', 'data', 'raw')))\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'data', 'processed')))\n",
    "\n",
    "\n",
    "from dataset import verifica_vazios, verifica_frequentes\n",
    "from data_processing import clean_text\n",
    "\n",
    "df[\"clean_title\"] = df[\"issue_title\"].apply(clean_text)\n",
    "df[\"clean_body\"] = df[\"body\"].apply(clean_text)\n",
    "\n",
    "\n",
    "media_palavras_url = df[\"issue_url\"].str.split().str.len().mean()\n",
    "media_palavras_title = df[\"clean_title\"].str.split().str.len().mean()\n",
    "media_palavras_body = df[\"clean_body\"].str.split().str.len().mean()\n",
    "\n",
    "print(f\"Média de palavras em url: {media_palavras_url}, em title: {media_palavras_title}, em body: {media_palavras_body}\")\n",
    "\n",
    "url_vazio = verifica_vazios(df,\"issue_url\")\n",
    "titles_vazio = verifica_vazios(df,\"clean_title\")\n",
    "body_vazio = verifica_vazios(df,\"clean_body\")\n",
    "\n",
    "palavras_frequentes_titles = verifica_frequentes(df,\"clean_title\", 20)\n",
    "print(f\"Palavras mais frequentes em titles: {palavras_frequentes_titles}\")\n",
    "palavras_frequentes_body = verifica_frequentes(df,\"clean_body\", 20)\n",
    "print(f\"Palavras mais frequentes em body: {palavras_frequentes_body}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68051a23",
   "metadata": {},
   "source": [
    "\n",
    "   **Análise descritiva dos dados processados**\n",
    "\n",
    "- Média de palavras para entender quantos tokens irão ser processados pela LLM\n",
    "    - Quantidade de células vazias em todas as colunas\n",
    "        - não teve resultado de células vazias, todas preenchidas\n",
    "    - Títulos - tamanhos médio de 5,4 palavras\n",
    "    - Body - Aproximadamente 28 palavras\n",
    "- Ideal para:\n",
    "    - Embeddings\n",
    "    - Chunking Leve\n",
    "    - RAG Eficiente(baixo custo e boa semântica)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d86188",
   "metadata": {},
   "source": [
    "**Criação de Texto final**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4683ea7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 683.41it/s, Materializing param=pooler.dense.weight]                               \n",
      "BertModel LOAD REPORT from: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_body</th>\n",
       "      <th>final_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>load addon issue error lib libc version glibc ...</td>\n",
       "      <td>load addon issue error lib libc version glibc ...</td>\n",
       "      <td>Title: load addon issue error lib libc version...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hcl accessibility yblocking ymas mas hcl makec...</td>\n",
       "      <td>user experience user depends screen reader get...</td>\n",
       "      <td>Title: hcl accessibility yblocking ymas mas hc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>issue issue issue issue issue issue issue issu...</td>\n",
       "      <td>attachments github com matisiekpl czekolada is...</td>\n",
       "      <td>Title: issue issue issue issue issue issue iss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>issue issue issue issue issue issue issue issu...</td>\n",
       "      <td>gitlo github trello board linked update issue ...</td>\n",
       "      <td>Title: issue issue issue issue issue issue iss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>issue issue issue issue issue issue issue issu...</td>\n",
       "      <td>attachments github com matisiekpl czekolada is...</td>\n",
       "      <td>Title: issue issue issue issue issue issue iss...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         clean_title  \\\n",
       "0  load addon issue error lib libc version glibc ...   \n",
       "1  hcl accessibility yblocking ymas mas hcl makec...   \n",
       "2  issue issue issue issue issue issue issue issu...   \n",
       "3  issue issue issue issue issue issue issue issu...   \n",
       "4  issue issue issue issue issue issue issue issu...   \n",
       "\n",
       "                                          clean_body  \\\n",
       "0  load addon issue error lib libc version glibc ...   \n",
       "1  user experience user depends screen reader get...   \n",
       "2  attachments github com matisiekpl czekolada is...   \n",
       "3  gitlo github trello board linked update issue ...   \n",
       "4  attachments github com matisiekpl czekolada is...   \n",
       "\n",
       "                                          final_text  \n",
       "0  Title: load addon issue error lib libc version...  \n",
       "1  Title: hcl accessibility yblocking ymas mas hc...  \n",
       "2  Title: issue issue issue issue issue issue iss...  \n",
       "3  Title: issue issue issue issue issue issue iss...  \n",
       "4  Title: issue issue issue issue issue issue iss...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#criação de nova coluna para texto final que será direcionado ao embedding\n",
    "df[\"final_text\"] = (\n",
    "    \"Title: \" + df[\"clean_title\"] +\n",
    "    \". Body: \" + df[\"clean_body\"]\n",
    ")\n",
    "\n",
    "#carregando o modelo que será usado, modelo rápido e leve para projeto OBS: Uso da CPU pois GPU esta ultrapassada para o modelo\n",
    "model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\", device=\"cpu\") #usando o paraphrasal para poder perguntar em portugues\n",
    "df[[\"clean_title\", \"clean_body\", \"final_text\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c24c0a4",
   "metadata": {},
   "source": [
    "**Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74f18f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 5/5 [00:02<00:00,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#listando o texto final em variavel para ser codificada\n",
    "texts = df[\"final_text\"].tolist()\n",
    "\n",
    "#realização do embedding pelo modelo escolhido\n",
    "embeddings = model.encode( \n",
    "    texts,\n",
    "    show_progress_bar= True\n",
    ")\n",
    "\n",
    "#criação do array em np \n",
    "embeddings = np.array(embeddings)\n",
    "print(embeddings.shape)\n",
    "\n",
    "#persistindo os dados em formato npy e csv para nao necessitar de conversao novamente\n",
    "np.save(\"embeddings.npy\",embeddings)\n",
    "df.to_csv(\"issue_processed.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f82ff1",
   "metadata": {},
   "source": [
    "**Busca Semântica**\n",
    "\n",
    "- Primeiramente utilizarei busca ingênua para este caso, mais rápido e ideal para projetos pequenos.\n",
    "    - será utilizado semelhança de cossenos, variância de -1 a 1, sendo 1 o mais próximo.\n",
    "    - comparação de vetores do embedding, modelo utilizado de dimensão 384\n",
    "- Após implementado e projeto funconando, irei dar updgrade para utilizar índice vetorial\n",
    "- Devido a arquitetura proposta relacionado aos tipos de perguntas para o Chatbot, será implementado:\n",
    "    - Busca semântica\n",
    "    - Top k\n",
    "    - Limiar de similiaridade\n",
    "    Dessa forma será possível a resposta de perguntas analtícas e explicativas, não somente localizadoras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84ddcf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #ler diretamente esta célula sem precisar converter novamente os arquivos no embedding\n",
    "import numpy as np\n",
    "\n",
    "embeddings = np.load(\"embeddings.npy\")\n",
    "df = pd.read_csv(\"issue_processed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142573a2",
   "metadata": {},
   "source": [
    "**Exemplo de pergunta e formato para query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58b58c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150,)\n",
      "[ 48  49  51  52  54  53  59  57  40  46  45  44  47  61  60  63  36  38\n",
      "  35   2  12   6   5   4  27  26  21  23  20  13  11  30  17  18  19  39\n",
      "  97 110 106 105 108 121 126 127 116 118 113 112 120  99 101 100 102  93\n",
      "  95  90  80  81  82  86  92  77  70  66 146 138 139 132 131 130 129 128\n",
      " 149  94  62  88  50  43  56  58   3  16  15  14   8   7   9  55  37  32\n",
      "  33  34  29  31  41  42  64 111  89  91  83  84  85  87  71  72  74  73\n",
      "  75  76  78  79 119 117 104 109  98  96 103  10  24  28  22  25  67  68\n",
      "  69  65 143 142 124 125 115 114 122 123 107 134 135 136 140 141 137 133\n",
      " 147 148 145 144   0   1]\n",
      "[  1   0 144]\n"
     ]
    }
   ],
   "source": [
    "from semantic_search import encode_query, cosine_similiarity_func\n",
    "\n",
    "pergunta = \"Quais sao os erros mais registrados no documento?\"\n",
    "query = encode_query(model, [pergunta])\n",
    "#print(query.shape)\n",
    "\n",
    "scores = cosine_similiarity_func(query,embeddings)\n",
    "scores = scores.flatten()\n",
    "scores_ordenados = np.argsort(scores)\n",
    "print(scores.shape)\n",
    "print(scores_ordenados)\n",
    "\n",
    "#fazendo busca top_k, depois refatorar montando em funcoes definidas\n",
    "# Atualizar o Readme e usar o topk como parametro para busca ao refatorar “Utilizamos top-k dinâmico para balancear cobertura semântica e precisão.”\n",
    "top_k = 3\n",
    "top_indices = scores_ordenados[-top_k:][::-1] #quero buscar os ultimos 20 valores e ordena-los em sequência maior para menor, pois np.arg retorna ordem crescente\n",
    "print(top_indices)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2040c41",
   "metadata": {},
   "source": [
    "**Implementando Limiar de similaridade**\n",
    "\n",
    " - Utilizei um limiar de similaridade cosseno ajustado empiricamente para garantir que apenas documentos semanticamente relevantes sejam utilizados como contexto para a LLM, reduzindo ruído e alucinação.\n",
    " - A princípio foi definido padrão = 0.30, após verifiquei de forma empirica os melhores valores para retornar respostas completas sem interferências de dados fora do padrão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "808f1706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'indice': np.int64(1), 'score': np.float32(0.6257697), 'text': 'Title: hcl accessibility yblocking ymas mas hcl makecode win edge title screen reader help javascript call function narrator focus moving expand side documentation button pressing enter collapse side documentation button. Body: user experience user depends screen reader get confused narrator focus retain expand side documentation button pressing enter collapse side documentation button test environment version build platform edge screen reader narrator repro steps navigate navigate micro bit section element select code control given navigate help control lying header section page select navigate javascript control select navigate various controls lying pane opened navigate pane opened select link listed verify narrator focus moving expand side documentation button pressing enter collapse side documentation button actual result narrator focus retain expand side documentation button pressing enter collapse side documentation button pressing enter collapse side documentation button narrator focus visible place moving expand button expected result narrator focus shift expand side documentation button pressing enter collapse side documentation button mas reference please refer attachment details mas call function visual focus indicator visible side documentation button zip'}, {'indice': np.int64(0), 'score': np.float32(0.58515), 'text': 'Title: load addon issue error lib libc version glibc found required usr local app taf fileserver fileserver bin src node modules images bindings linux binding node. Body: load addon issue error lib libc version glibc found required usr local app taf fileserver fileserver bin src node modules images bindings linux binding node glibc'}, {'indice': np.int64(144), 'score': np.float32(0.5227478), 'text': 'Title: issue issue issue issue issue issue issue issue issue issue issue issue issue issue issue issue issue issue issue issue issue issu. Body: gitlo github trello board linked update issue tracker sync board via trello create github issues add card corresponding column issue created github add issue comment github issues comment always card comment issue close opened issues move issue cards close list course reopen dragging close list close issue close pull requests move cards close list important merge via trello close add custom columns need column sync github label default columns set default columns please help keeping place also add custom columns enhance work flow add column dashboard settings owner project visit update settings want let know mail gitlo oursky com attachments github com matisiekpl czekolada issues'}]\n"
     ]
    }
   ],
   "source": [
    "#definir o limiar, fazer um loop retornando os melhores índices\n",
    "limiar = 0.30\n",
    "lista_final = []\n",
    "\n",
    "for i in range(len(top_indices)):\n",
    "    if scores[top_indices[i]] > limiar:\n",
    "        lista_final.append({\"indice\" : top_indices[i] ,\"score\" :scores[top_indices[i]], \"text\" : df.iloc[top_indices[i]][\"final_text\"]})\n",
    "\n",
    "print(lista_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbd2c9c",
   "metadata": {},
   "source": [
    "**Construção do contexto**\n",
    "\n",
    "- Utilizar dos dados retornados a partir do limiar e construir os textos mais similares para servir de entrada para LLM\n",
    "- estou retornando em texto todos os documentos analisados que passaram pelo limiar, poderia restringir a quantidade caso o número de tokens a ser utilizado na LLM seja limitado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4df7eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Contexto 1 | Similaridade: 0.626]\n",
      "Title: hcl accessibility yblocking ymas mas hcl makecode win edge title screen reader help javascript call function narrator focus moving expand side documentation button pressing enter collapse side documentation button. Body: user experience user depends screen reader get confused narrator focus retain expand side documentation button pressing enter collapse side documentation button test enviro\n",
      "\n",
      "[Contexto 2 | Similaridade: 0.585]\n",
      "Title: load addon issue error lib libc version glibc found required usr local app taf fileserver fileserver bin src node modules images bindings linux binding node. Body: load addon issue error lib libc version glibc found required usr local app taf fileserver fileserver bin src node modules images bindings linux binding node glibc\n",
      "\n",
      "[Contexto 3 | Similaridade: 0.523]\n",
      "Title: issue issue issue issue issue issue issue issue issue issue issue issue issue issue issue issue issue issue issue issue issue issu. Body: gitlo github trello board linked update issue tracker sync board via trello create github issues add card corresponding column issue created github add issue comment github issues comment always card comment issue close opened issues move issue cards clos\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_CHARS = 400  # por documento\n",
    "\n",
    "lista_final = sorted(\n",
    "    lista_final,\n",
    "    key=lambda x: x[\"score\"],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "contexto = \"\"\n",
    "limite = 3\n",
    "\n",
    "if len(lista_final) <= limite:\n",
    "    for i in range(len(lista_final)):\n",
    "        text_limited = lista_final[i][\"text\"][:MAX_CHARS]\n",
    "\n",
    "        contexto += (\n",
    "            f\"[Contexto {i+1} | \"\n",
    "            f\"Similaridade: {lista_final[i]['score']:.03f}]\\n\"\n",
    "            f\"{text_limited}\\n\\n\"\n",
    "        )\n",
    "\n",
    "else:\n",
    "    for i in range(limite):\n",
    "        text_limited = lista_final[i][\"text\"][:MAX_CHARS]\n",
    "\n",
    "        contexto += (\n",
    "            f\"[Contexto {i+1} | \"\n",
    "            f\"Similaridade: {lista_final[i]['score']:.03f}]\\n\"\n",
    "            f\"{text_limited}\\n\\n\"\n",
    "        )\n",
    "\n",
    "print(contexto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9d310f",
   "metadata": {},
   "source": [
    "**Integrando LLM**\n",
    "\n",
    "- Estou usando Gemini devido a facilidade de obtenção de uma chave gratuita para estudantes\n",
    "- A LLM irá receber as informações retiradas do Dataset \n",
    "- Será utilizado engenharia de prompt para qualificar a análise da LLM\n",
    "- No projeto será implantado um scan de input para o usuário digitar sua propria chave api gemini e testar o programa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3ca6980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro ao gerar resposta: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\nPlease retry in 14.977411114s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'model': 'gemini-2.0-flash', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.0-flash', 'location': 'global'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '14s'}]}}\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join('..', 'src', 'prompts')))\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src', 'llm')))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from rag_prompts import build_prompts\n",
    "from gemini_client import generate_answer\n",
    "\n",
    "load_dotenv()\n",
    "chave_api = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "prompt = build_prompts(contexto, pergunta)\n",
    "#resposta = generate_answer(prompt)\n",
    "\n",
    "#print(resposta)\n",
    "\n",
    "# Tente um prompt simples para validar a chave\n",
    "teste = generate_answer(\"Olá, você está funcionando?\")\n",
    "print(teste)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
