{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a13bdecc",
   "metadata": {},
   "source": [
    "**# Importanto o Dataset e verificando o conteúdo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7709c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rafae\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "c:\\Users\\rafae\\Desktop\\Portifolio\\AI_ASSISTANT_GITHUB\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           issue_url  \\\n",
      "0  \"https://github.com/zhangyuanwei/node-images/i...   \n",
      "1     \"https://github.com/Microsoft/pxt/issues/2543\"   \n",
      "2  \"https://github.com/MatisiekPL/Czekolada/issue...   \n",
      "3  \"https://github.com/MatisiekPL/Czekolada/issue...   \n",
      "4  \"https://github.com/MatisiekPL/Czekolada/issue...   \n",
      "\n",
      "                                         issue_title  \\\n",
      "0  can't load the addon. issue to: https://github...   \n",
      "1  hcl accessibility a11yblocking a11ymas mas4.2....   \n",
      "2  issue 1265: issue 1264: issue 1261: issue 1260...   \n",
      "3  issue 1266: issue 1263: issue 1262: issue 1259...   \n",
      "4  issue 1288: issue 1285: issue 1284: issue 1281...   \n",
      "\n",
      "                                                body  \n",
      "0  can't load the addon. issue to: https://github...  \n",
      "1  user experience: user who depends on screen re...  \n",
      "2  ┆attachments: <a href= https:& x2f;& x2f;githu...  \n",
      "3  gitlo = github x trello\\n---\\nthis board is no...  \n",
      "4  ┆attachments: <a href= https:& x2f;& x2f;githu...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rafae\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords') #carrego stopwrds para filtragem NLP\n",
    "\n",
    "# Adiciona o caminho da pasta onde o arquivo 'dataset.py' está localizado\n",
    "# Isso pula a necessidade de mencionar a pasta com hífen no import\n",
    "caminho_raw = os.path.abspath(os.path.join('..', 'data', 'raw'))\n",
    "if caminho_raw not in sys.path:\n",
    "    sys.path.append(caminho_raw)\n",
    "\n",
    "from dataset import carregar_dados\n",
    "\n",
    "df = carregar_dados()\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0010a67a",
   "metadata": {},
   "source": [
    "**Processamento e limpeza dos dados**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "427d4c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média de palavras em url: 1.0, em title: 5.459, em body: 28.4455\n",
      "Resultados para 'issue_url':\n",
      "- Valores Nulos (NaN): 0\n",
      "- Textos Vazios/Espaços: 0\n",
      "Resultados para 'clean_title':\n",
      "- Valores Nulos (NaN): 0\n",
      "- Textos Vazios/Espaços: 99\n",
      "Resultados para 'clean_body':\n",
      "- Valores Nulos (NaN): 0\n",
      "- Textos Vazios/Espaços: 11\n",
      "Palavras mais frequentes em titles: [('issu', 352), ('add', 151), ('bug', 137), ('found', 79), ('error', 49), ('test', 43), ('use', 39), ('api', 39), ('help', 34), ('fix', 34), ('new', 33), ('need', 26), ('log', 24), ('run', 23), ('list', 22), ('get', 20), ('set', 19), ('line', 17), ('typo', 17), ('task', 16)]\n",
      "Palavras mais frequentes em body: [('close', 1244), ('add', 1102), ('issues', 927), ('columns', 714), ('list', 585), ('comment', 558), ('column', 542), ('trello', 529), ('update', 396), ('default', 387), ('settings', 378), ('via', 376), ('sync', 373), ('move', 369), ('custom', 367), ('board', 358), ('card', 357), ('attachments', 355), ('cards', 353), ('matisiekpl', 352)]\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join('..', 'data', 'raw')))\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'data', 'processed')))\n",
    "\n",
    "\n",
    "from dataset import verifica_vazios, verifica_frequentes\n",
    "from data_processing import clean_text\n",
    "\n",
    "df[\"clean_title\"] = df[\"issue_title\"].apply(clean_text)\n",
    "df[\"clean_body\"] = df[\"body\"].apply(clean_text)\n",
    "\n",
    "\n",
    "media_palavras_url = df[\"issue_url\"].str.split().str.len().mean()\n",
    "media_palavras_title = df[\"clean_title\"].str.split().str.len().mean()\n",
    "media_palavras_body = df[\"clean_body\"].str.split().str.len().mean()\n",
    "\n",
    "print(f\"Média de palavras em url: {media_palavras_url}, em title: {media_palavras_title}, em body: {media_palavras_body}\")\n",
    "\n",
    "url_vazio = verifica_vazios(df,\"issue_url\")\n",
    "titles_vazio = verifica_vazios(df,\"clean_title\")\n",
    "body_vazio = verifica_vazios(df,\"clean_body\")\n",
    "\n",
    "palavras_frequentes_titles = verifica_frequentes(df,\"clean_title\", 20)\n",
    "print(f\"Palavras mais frequentes em titles: {palavras_frequentes_titles}\")\n",
    "palavras_frequentes_body = verifica_frequentes(df,\"clean_body\", 20)\n",
    "print(f\"Palavras mais frequentes em body: {palavras_frequentes_body}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68051a23",
   "metadata": {},
   "source": [
    "\n",
    "   **Análise descritiva dos dados processados**\n",
    "\n",
    "- Média de palavras para entender quantos tokens irão ser processados pela LLM\n",
    "    - Quantidade de células vazias em todas as colunas\n",
    "        - não teve resultado de células vazias, todas preenchidas\n",
    "    - Títulos - tamanhos médio de 5,4 palavras\n",
    "    - Body - Aproximadamente 28 palavras\n",
    "- Ideal para:\n",
    "    - Embeddings\n",
    "    - Chunking Leve\n",
    "    - RAG Eficiente(baixo custo e boa semântica)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
